{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import optimizers\n",
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tf.keras.datasets.cifar10\n",
    "(x_train, y_train), (x_test, y_test) = data.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 32, 32, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_label_names =['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 59,  62,  63],\n",
       "        [ 43,  46,  45],\n",
       "        [ 50,  48,  43],\n",
       "        ...,\n",
       "        [158, 132, 108],\n",
       "        [152, 125, 102],\n",
       "        [148, 124, 103]],\n",
       "\n",
       "       [[ 16,  20,  20],\n",
       "        [  0,   0,   0],\n",
       "        [ 18,   8,   0],\n",
       "        ...,\n",
       "        [123,  88,  55],\n",
       "        [119,  83,  50],\n",
       "        [122,  87,  57]],\n",
       "\n",
       "       [[ 25,  24,  21],\n",
       "        [ 16,   7,   0],\n",
       "        [ 49,  27,   8],\n",
       "        ...,\n",
       "        [118,  84,  50],\n",
       "        [120,  84,  50],\n",
       "        [109,  73,  42]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[208, 170,  96],\n",
       "        [201, 153,  34],\n",
       "        [198, 161,  26],\n",
       "        ...,\n",
       "        [160, 133,  70],\n",
       "        [ 56,  31,   7],\n",
       "        [ 53,  34,  20]],\n",
       "\n",
       "       [[180, 139,  96],\n",
       "        [173, 123,  42],\n",
       "        [186, 144,  30],\n",
       "        ...,\n",
       "        [184, 148,  94],\n",
       "        [ 97,  62,  34],\n",
       "        [ 83,  53,  34]],\n",
       "\n",
       "       [[177, 144, 116],\n",
       "        [168, 129,  94],\n",
       "        [179, 142,  87],\n",
       "        ...,\n",
       "        [216, 184, 140],\n",
       "        [151, 118,  84],\n",
       "        [123,  92,  72]]], dtype=uint8)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3],\n",
       "       [8],\n",
       "       [8],\n",
       "       ...,\n",
       "       [5],\n",
       "       [1],\n",
       "       [7]], dtype=uint8)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1966a882ba8>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "i = np.random.randint(1, len(x_train))\n",
    "plt.imshow(x_train[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 10)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 32, 32, 3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#normalization\n",
    "x_train = x_train /255\n",
    "x_test = x_test /255\n",
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 32, 32, 3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (32,32,3)\n",
    "batch_size =100\n",
    "epochs =10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model  = Sequential()\n",
    "\n",
    "model.add(Conv2D(20,(5,5),activation = 'relu',input_shape = input_shape))\n",
    "model.add(MaxPooling2D(pool_size = (2, 2),strides =  (2, 2)))\n",
    "\n",
    "model.add(Conv2D(50,(5,5),activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size = (2, 2),strides =  (2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(500,activation = 'relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(84,activation = 'relu'))\n",
    "\n",
    "model.add(Dense(10,activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 28, 28, 20)        1520      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 14, 14, 20)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 10, 10, 50)        25050     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 50)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1250)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 500)               625500    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 84)                42084     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                850       \n",
      "=================================================================\n",
      "Total params: 695,004\n",
      "Trainable params: 695,004\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(loss= 'categorical_crossentropy',optimizer = 'adam' , metrics = ['accuracy'])\n",
    "model.compile(loss = \"categorical_crossentropy\", optimizer = SGD(lr = 0.01),metrics = [\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "500/500 [==============================] - 88s 175ms/step - loss: 2.1712 - accuracy: 0.1868 - val_loss: 1.9685 - val_accuracy: 0.2923\n",
      "Epoch 2/10\n",
      "500/500 [==============================] - 82s 163ms/step - loss: 1.9253 - accuracy: 0.3007 - val_loss: 1.8023 - val_accuracy: 0.3551\n",
      "Epoch 3/10\n",
      "500/500 [==============================] - 82s 163ms/step - loss: 1.7567 - accuracy: 0.3693 - val_loss: 1.6021 - val_accuracy: 0.4285\n",
      "Epoch 4/10\n",
      "500/500 [==============================] - 82s 164ms/step - loss: 1.6165 - accuracy: 0.4154 - val_loss: 1.5104 - val_accuracy: 0.4612\n",
      "Epoch 5/10\n",
      "500/500 [==============================] - 82s 164ms/step - loss: 1.5266 - accuracy: 0.4469 - val_loss: 1.5398 - val_accuracy: 0.4596\n",
      "Epoch 6/10\n",
      "500/500 [==============================] - 81s 162ms/step - loss: 1.4649 - accuracy: 0.4713 - val_loss: 1.4050 - val_accuracy: 0.4952\n",
      "Epoch 7/10\n",
      "500/500 [==============================] - 81s 162ms/step - loss: 1.4156 - accuracy: 0.4933 - val_loss: 1.3852 - val_accuracy: 0.5075\n",
      "Epoch 8/10\n",
      "500/500 [==============================] - 83s 165ms/step - loss: 1.3713 - accuracy: 0.5077 - val_loss: 1.3127 - val_accuracy: 0.5326\n",
      "Epoch 9/10\n",
      "500/500 [==============================] - 82s 164ms/step - loss: 1.3352 - accuracy: 0.5243 - val_loss: 1.3159 - val_accuracy: 0.5310\n",
      "Epoch 10/10\n",
      "500/500 [==============================] - 82s 165ms/step - loss: 1.3024 - accuracy: 0.5365 - val_loss: 1.2611 - val_accuracy: 0.5524\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(x_train, y_train,batch_size=batch_size,epochs=epochs,verbose=1,validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  61 % Accuracy on the test set which used as the validation data with LeNet without data Augmentation.Using Adam optimiser.\n",
    "# 55.24% using SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nows lets compare this accuracy, with the accuracy after Data augmentation ,by flipping the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 32, 32, 3)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import LearningRateScheduler, TensorBoard\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_aug = ImageDataGenerator(horizontal_flip=True,width_shift_range=0.125,height_shift_range=0.125,fill_mode='constant',cval=0.)\n",
    "data_aug.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-23-b2a352a54b3f>:2: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "Epoch 1/10\n",
      "391/391 [==============================] - 59s 151ms/step - loss: 1.4656 - accuracy: 0.4699 - val_loss: 1.2720 - val_accuracy: 0.5483\n",
      "Epoch 2/10\n",
      "391/391 [==============================] - 82s 210ms/step - loss: 1.4273 - accuracy: 0.4850 - val_loss: 1.2656 - val_accuracy: 0.5498\n",
      "Epoch 3/10\n",
      "391/391 [==============================] - 82s 209ms/step - loss: 1.4166 - accuracy: 0.4944 - val_loss: 1.2931 - val_accuracy: 0.5395\n",
      "Epoch 4/10\n",
      "391/391 [==============================] - 84s 215ms/step - loss: 1.3943 - accuracy: 0.5004 - val_loss: 1.2274 - val_accuracy: 0.5668\n",
      "Epoch 5/10\n",
      "391/391 [==============================] - 84s 215ms/step - loss: 1.3727 - accuracy: 0.5099 - val_loss: 1.2570 - val_accuracy: 0.5548\n",
      "Epoch 6/10\n",
      "391/391 [==============================] - 83s 212ms/step - loss: 1.3620 - accuracy: 0.5141 - val_loss: 1.2101 - val_accuracy: 0.5696\n",
      "Epoch 7/10\n",
      "391/391 [==============================] - 82s 209ms/step - loss: 1.3441 - accuracy: 0.5196 - val_loss: 1.2144 - val_accuracy: 0.5707\n",
      "Epoch 8/10\n",
      "391/391 [==============================] - 85s 218ms/step - loss: 1.3319 - accuracy: 0.5272 - val_loss: 1.1948 - val_accuracy: 0.5812\n",
      "Epoch 9/10\n",
      "391/391 [==============================] - 82s 209ms/step - loss: 1.3141 - accuracy: 0.5320 - val_loss: 1.1843 - val_accuracy: 0.5851\n",
      "Epoch 10/10\n",
      "391/391 [==============================] - 82s 209ms/step - loss: 1.3049 - accuracy: 0.5387 - val_loss: 1.1697 - val_accuracy: 0.5877\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1966a8f09e8>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(data_aug.flow(x_train, y_train,batch_size=batch_size),steps_per_epoch=391,\n",
    "                        epochs=epochs,validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 58.77 with data augmentation. This has increased the accuracy by about 3%.\n",
    "# This can be improved . Check the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_aug2 = ImageDataGenerator(featurewise_center=False,  samplewise_center=False,  \n",
    "        featurewise_std_normalization=False, \n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        zoom_range = 0.1, # Randomly zoom image \n",
    "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=False,  # randomly flip images\n",
    "        vertical_flip=False)  # randomly flip images\n",
    "\n",
    "\n",
    "data_aug2.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 1.2695 - accuracy: 0.5476 - val_loss: 1.1664 - val_accuracy: 0.5831\n",
      "Epoch 2/10\n",
      "391/391 [==============================] - 86s 220ms/step - loss: 1.2535 - accuracy: 0.5558 - val_loss: 1.1336 - val_accuracy: 0.5957\n",
      "Epoch 3/10\n",
      "391/391 [==============================] - 86s 220ms/step - loss: 1.2374 - accuracy: 0.5614 - val_loss: 1.1383 - val_accuracy: 0.5980\n",
      "Epoch 4/10\n",
      "391/391 [==============================] - 86s 220ms/step - loss: 1.2235 - accuracy: 0.5666 - val_loss: 1.0956 - val_accuracy: 0.6114\n",
      "Epoch 5/10\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 1.2143 - accuracy: 0.5702 - val_loss: 1.0886 - val_accuracy: 0.6171\n",
      "Epoch 6/10\n",
      "391/391 [==============================] - 85s 218ms/step - loss: 1.2013 - accuracy: 0.5739 - val_loss: 1.0973 - val_accuracy: 0.6121\n",
      "Epoch 7/10\n",
      "391/391 [==============================] - 86s 220ms/step - loss: 1.1938 - accuracy: 0.5768 - val_loss: 1.1953 - val_accuracy: 0.5827\n",
      "Epoch 8/10\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 1.1886 - accuracy: 0.5823 - val_loss: 1.0819 - val_accuracy: 0.6175\n",
      "Epoch 9/10\n",
      "391/391 [==============================] - 85s 218ms/step - loss: 1.1676 - accuracy: 0.5874 - val_loss: 1.0394 - val_accuracy: 0.6353\n",
      "Epoch 10/10\n",
      "391/391 [==============================] - 85s 218ms/step - loss: 1.1656 - accuracy: 0.5851 - val_loss: 1.0759 - val_accuracy: 0.6245\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x196094a15c0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(data_aug2.flow(x_train, y_train,batch_size=batch_size),steps_per_epoch=391,\n",
    "                        epochs=epochs,validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accuracy has increased to 62.45% after data augmentation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
